% !TEX TS-program = lualatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt,draft]{article} % use larger type; default would be 10pt
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....


\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy
%\renewcommand{\headrulewidth}{0pt} % customise the layout...
%\lhead{}\chead{}\rhead{}
%\lfoot{}\cfoot{\thepage}\rfoot{}

\usepackage[
    style=numeric-comp,
    sortlocale=en_AU,
    url=false, 
    doi=true,
    eprint=false
]{biblatex}
\addbibresource{Cognitive-robotics.bib}


%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{A curious reason for autonomous space exploration: Project proposal (DRAFT)}
\author{Sophie Taylor n6362575}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Abstract}
We discuss the notion of a cognitive architecture for an autonomous space exploration robot motivated by curiosity. We propose to implement an agent with such an architecture and investigate some of its properties.



\section{Literature review}
\subsection{Reinforcement learning}
Reinforcement learning algorithms are training algorithms, designed to maximise the expected reward from a (possibly continuous) sequence of actions of an agent in some environment. One notable class of reinforcement learning algorithms are episodic in nature, exemplified in \autocite{blundell_model-free_2016,pritzel_neural_2017}.Typically, rewards are external, such as some fitness measure. Internal rewards, on the other hand, can be considered as \emph{motivations}. One such motivation is curiosity.
\subsection{Artificial curiosity}
\textcite{hutchison_coherence_2011} introduces the concept of \emph{compression coherence}, as a proposed measure for curiosity. It evaluates sequential observations in terms of how adding new data increases the compression ratio, signifying an increase in model accuracy, or understanding. \textcite{graziano_artificial_2011} builds on and generalises this to arbitrary adaptive compressors, motivated by an autonomous space science robot.
\subsection{Compression}
\textcite{steinruecken_lossless_2014} is a mathematically rigorous approach to the fundamentals of lossless data compression, for compressing objects such as sequences, sets, and multisets. \textcite{steinruecken_compressing_2016} expands on compressing other combinitorial objects. \textcite{mahoney_adaptive_2005} introduces using adaptive weighting of compression models via primitive neural networks.
\subsection{Information theory}
\subsection{Induction}
Induction can be described as the problem of how we can justifiably extrapolate observations in order to make predictions. \textcite{rathmanner_philosophical_2011} is an exposition on Solomonoff induction, a mathematical formalisation of Occam's Razor, and Epicurus' Multiple Explainations theory, combined with Kolmogorov complexity in order to predict the next symbol of an observed sequence.
\subsection{Model building}
There are many approaches to building predictive models of data. Some of the techniques particularly relevant to our setting are Gaussian processes \autocite{duvenaud_automatic_2014} and genetic algorithms such as used in Eureqa \autocite{schmidt_distilling_2009}.
\subsubsection{Probabilistic graphical models}
\subsection{Autonomous agents}
\subsection{Cognitive architecture}
\textcite{frank_curiosity_2014} describes an architecture for a robotic humanoid, designed for low-latency motion planning and control of a large number of continuous actuators with a high degree of freedom.

\subsection{Modal logic}



\section{Experiments and investigation}
The goal is to experiment with how curiosity can be exploited throughout all aspects of a robotic cosmonaut's design, not just its cognitive architecture. As such, we propose to first build a simple, OODA-style agent in order to experiment with various design decisions.
\subsection{Can data compressed during learning be decompressed losslessly, within reason?}
While the main goal of a robotic scientist is to understand the world, we would like to see the data supporting its beliefs; in addition to verifying model correctness, the data may be valuable in its own right. Thus, it is desirable for the data to be transmitted to human scientists such that they can actually retreive it.
\subsection{Can the compressed data be sent to human scientists without further processing?}
Is the data compressed by the agent in a format and size suitable for transmission and use by human scientists, or must it be further compressed? This serves as a sanity check on the capability of the agent to produce a meaningful model, as the better the model is at predicting the data, the better the compression.
\subsection{Can meaningful models be extracted?}
As the goal of science is to build models of the universe in order to deepen our understanding, we would like to be able to understand the learned models. Compared to non-trivial deep neural nets, which are notoriously opaque to human understanding, it is desirable for the agent to create sparse models in the form of some symbolic language.
\subsection{Background knowledge versus tabula rasa}
Investigate the performance of the agent when given basic knowledge of physics, such as Newton's laws and some constants.

\subsection{Can we describe our hypotheses, actions and observations in modal logic?}
Formalising our thought history in logic may help to find better models. For example, we may be able to pose our questions to an SMT solver such as Z3\cite{de_moura_z3_2008}.



\section{Plan}
\subsection{Questions to be resolved for implementation}
 The following questions must be answered before implementation can begin.
\subsubsection{How can curiosity be usefully integrated as an intrinsic motivation?}
While there are existing cognitive architectures with the ability to specify an arbitrary reward, they may be poorly suited for curious agents whose primary purpose is to build models, rather than to exploit models. \textcite{frank_curiosity_2014} seems to come closest to what we desire.
\subsubsection{How should decisions be made regarding what action to take?}
Possible options include neural networks, or an topological approach such as \cite{lakkaraju_identifying_2016}. 
\subsubsection{What reinforcement learning algorithm should be chosen?}
Episodic learning is a seemingly appropriate technique to employ, enabling an agent to "look back" at past episodes in order to . The specific choice depends on the technique used for deciding actions.
\subsubsection{How can the agent move through the action space?}
By performing experiments. Thus, actions correspond to experiments with specific control variables.
\subsubsection{How can the agent perform experiments?}
Control variables for continuous data corresponds to the base topological space of a fibre bundle. The time series is the fibre at that point.
\subsubsection{What data is to be used for testing?}
Generated data for continuous models, such as damped oscillators, and a thrown projectile, including variations with messy terms such as drag. 
\subsubsection{How to build models?}
The two chief candidates are \autocite{duvenaud_automatic_2014} and \autocite{schmidt_distilling_2009}. \autocite{schmidt_distilling_2009} has the advantage of being implemented as a COTS software package, free for academic use, whereas \autocite{duvenaud_automatic_2014} may allow more flexibility and not having to spend time learning the API of the software package. Of course, the techniques in \autocite{schmidt_distilling_2009} may also be reimplemented, but this may not be warranted.
\subsubsection{When should we build new models?}
Should building new models be part of the compressor that happens automatically, or should it be an action exposed to the reinforcement learning algorithm?

\subsection{Implementation language}
The design will be implemented in Haskell. The two chief options were Haskell and Python. While Python has the advantage of being easy to start protoyping with, and native TensorFlow bindings, it is much more difficult to debug and refactor than Haskell. Haskell has the advantage of being a lazy functional programming language, meaning mathematically-oriented software is much easier to read and write. What cemented Haskell as the superior choice due to the author's familarity with the language and it's various frameworks.

\subsection{Outline of design}
The agent loop's is projected to look like the following:

\subsubsection{Choose experiments to perform}
This will be done by the chosen high-level control structure, trained by the chosen reinforcement learning algorithm.
\subsubsection{Get observation}
Execution of the chosen action. This may be, for example, simply executing the simulated gathering function with new parameters.
\subsubsection{Create hypothesis, or model}
This step depends on whether model creation is an explicit action, or merely extracting the model from the previous compression step.
\subsubsection{Test hypothesis plausibility against a suitable prior}
\subsubsection{Evaluate observed data against model}
\subsubsection{Subtract observed data from model prediction and encode residual}
\subsubsection{Compress data}
\subsubsection{Store or transmit model and compressed data}
\subsubsection{Evaluate coherence progress}

\subsection{Timeline}
Most of questions to be resolved should be answered by 1/6/2018. The implementation of the experimental functions (e.g. the damped oscillator) can begin immediately after.
\printbibliography 
\end{document}
