% !TEX TS-program = lualatex
% !TEX encoding = UTF-8 Unicode


\documentclass[11pt]{report} 
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)


%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} 


\usepackage{graphicx} % support the \includegraphics command and options

\usepackage{fancyref}

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

\usepackage[
    style=numeric-comp,
    sortlocale=en_AU,
    url=false, 
    doi=true,
    eprint=false
]{biblatex}
\addbibresource{Cognitive-robotics.bib}


%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!


\title{Artificial curiosity in robotic cosmonauts \\ \large Progress report}

\author{Sophie Taylor n6362575}
\date{8/6/2018} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\chapter*{Abstract}
We discuss the notion of a cognitive architecture for an autonomous space exploration robot motivated by curiosity. We propose to implement an agent with such an architecture and investigate some of its properties.

\chapter{Introduction}
Space exploration is hard. One of the reasons for this is the distance. Despite the speed of light being incredibly fast to us, the universe is very big. The radio propagation time between Earth and a rover on Mars, for example, can be anywhere between 4 minutes and 24 minutes, depending on the time of (each planet's) year. The distance also means enormous free space path loss --- For example, an S band transmission experiences between 257 and 274 dB of loss. This greatly reduces the Shannon capacity of the communication channel, reducing the amount of science data which can be returned.

Autonomy is crucial in overcoming these barriers. Autonomous control reduces the need for detailed command constructions; we can create relatively high level, autonomous GNC controllers which we issue high-level instructions such as ``Go to that interesting rock", and it can be expected to avoid getting stuck in a crater enroute without any human interaction. To reduce the amount of uplink bandwidth required, we can also preprocess the collected data, such as creating and transmitting only a histogram of the data, rather than the individual datapoints; we can also compress it. Combining these two functions is the goal this project is working towards.

It is desirable to make our robotic cosmonauts as autonomous as possible, so ideally we could just instruct them to ``go do some science and report any interesting discoveries". But how can we mathematically model ``Go do some science", or, for that matter, ``interesting discovery"?
There are two main, complimentary approaches to the creation of autonomous systems: Control theory, which is suited to low-level systems when both the plant and desired behaviour are relatively easy to mathematically model, and the desired behaviour is not vastly more abstract than the plant; and so-called artificial intelligence, which is suitable when we find great difficulty in modeling the plant, but can still describe the desired behaviour, which may be extremely high level.

\section{Artificial curiosity}
Let's describe science as ``systematic curiosity". But what is curiosity? We may describe it as acting in order to maximise the interestingness of our observations. We aren't interested in things which are completely predictable, nor about things which appear completely random. When we see probability, particularly in its Bayesian interpretation, turn up in abstract concepts, such as ``interaction", ``understanding" or ``interesting", it is helpful to turn to information theory. So, let us phrase it in such terms: interestingness is a nonmonotonic measure of similarity to previously understood (or modeled) information.

Compression, the study of which is field of information theory, works by finding and exploiting similarity in the given data by creating probabilistic models. This is exactly why we want the data - to study it to discover and model the physical processes which create it. This means we can use compression to formalise curiosity, in the form of, for example, coherence progress. Roughly speaking, this is the derivative of how effective our models are as we add new data: If two consecutive results are very efficiently compressed, then the difference in effectiveness is low, thus coherence progress is low; if two consecutive results are poorly understood and modeled, then they will not compress well, and the change in effectiveness is low as well. But if the adding of a new result allows the discovery of a new, more effective model, then the difference is large. Thus, we can see coherence progress captures the nonmonotonic nature of interestingness.

\section{Project goals}
This provides a large number of interesting research questions in several different topics, particularly on the nature of reusing the probabilistic models. Can we use models of craters, for example, to assist in path planning? 

Alternatively, questions relating to compression itself: How can we design our compression schemes to maximise their effectiveness in the context of artificial curiosity? Are the resulting models and compressed data in a suitable format for transmitting to human scientists, without additional processing? Most extant compression schemes don't actively ask for additional data (meaning, in our case, perform more experiments) to improve its models. Further, most of the models are nonphysical (e.g. acausal, n-gram based, and so on). As such, we are investigating pro-active, physically-based compression schemes for use in autonomous agents.




\chapter{Literature review}

\section{Reinforcement learning}
Reinforcement learning algorithms are training algorithms, designed to maximise the expected reward from a (possibly continuous) sequence of actions of an agent in some environment. One notable class of reinforcement learning algorithms are episodic in nature, exemplified in \autocite{blundell_model-free_2016,pritzel_neural_2017}.Typically, rewards are extrinsic, such as some fitness measure. Intrinsic rewards, on the other hand, can be considered as \emph{motivations}. One such motivation is curiosity. Extrinsic and intrinsic rewards can generally be combined arbitrarily.


\section{Artificial curiosity}
The notion of "interestingness" has only recently been formalised in a satisfactory manner. Compression progress, as outlined by \textcite{pezzulo_driven_2009}, is an information-theoretic approach in which the reward is a function of the differential progress some compression algorithm makes in adapting to its (generally streaming) input. \textcite{graziano_artificial_2011} expound on this in the context of space science.


\section{Compression}
\subsection{Lossless compression}
Compression comes in two flavours: Lossy compression, where we allow errors in the reconstructed data to achieve greater compression ratios, and lossless compression, where we don't. Lossy compression is typically used for audio and visual data, primarily for entertainment purposes. We shall ignore lossy compression, and focus solely on lossless compression. 

For decades, lossless compression research was mostly ad~hoc and focused on heuristics and intuition which combined both modeling and coding, such as the landmark development of the LZ77 by \textcite{ziv_universal_1977}. \textcite{steinruecken_lossless_2014} provided a rigorous approach to the fundamentals of lossless data compression from first principles, for compressing basic mathematical objects such as sequences, sets, and multisets. \textcite{steinruecken_compressing_2016} then expanded on compressing other combinitorial objects. 

\subsection{Adaptive compression}
Adaptive compression, of which our compression scheme necessarily falls under, is when the probability model of the data to be compressed isn't known in advance, and must be learned during the compression process itself \cite{steinruecken_lossless_2014}. A simple example is progressively learning a histogram of the data, and assigning the probability of a symbol being the frequency in which it has appeared so far, also called a 0-gram model \cite{steinruecken_lossless_2014}.
\textcite{mahoney_adaptive_2005} introduces using adaptive weighting of compression models via primitive neural networks.

\subsection{Context sensitive compression}
A model is said to be \emph{context sensitive} when the probability distribution for a symbol depends on surrounding symbols. For example, in English text, after a "q" appears, the next symbol is much more likely to be a "u" than a "z". An \emph{n-gram} model is when a symbol distribution depends on the $n$ symbols occuring immediately prior to it \cite{steinruecken_lossless_2014}. As we shall be focusing on physical processes, which are typically \emph{continuous}, our compression modeling must clearly be context sensitive. 

\section{Probabilistic modeling}
Given a perfectly accurate probabilistic model of some data, we can optimally compress it (to within 2 bits) via arithmetic coding to its information content \cite{steinruecken_lossless_2014}. Thus, compression research mostly focuses on improving modeling techniques. 
\subsection{Model building}
There are many approaches to building predictive models of data. Some of the techniques particularly relevant to our setting are Gaussian processes \autocite{duvenaud_automatic_2014} and genetic algorithms such as used in Eureqa \autocite{schmidt_distilling_2009}.
\subsection{Probabilistic graphical models}
Probabilistic graphical modeling is the study of probabilistic modeling using the tools of graph theory. This can drastically simplify the resultant models, increasing both clarity and computational efficiency. \textcite{koller_probabilistic_2009} is an introductory monograph on the subject, covering varying types of models. \textcite{xiang_probabilistic_2002} studies them in the context of intelligent agents.
\subsection{Induction}
Induction can be described as the problem of how we can justifiably extrapolate observations in order to make predictions. \textcite{rathmanner_philosophical_2011} provide an exposition on Solomonoff induction, a mathematical formalisation of Occam's Razor, and Epicurus' Multiple Explainations theory, combined with Kolmogorov complexity in order to predict the next symbol of an observed sequence.

\section{Information theory}
\subsection{Coding theory}
Coding theory is the study of codes, sequences or other combinations of symbols from an alphabet which are imbued with meaning. Source coding is the study of transforming one code to another in order to reduce the amount of information needed for the representation of its contents. In the context of compression, there are two major classes of codes: Dictionary codes, which are also known as substitution codes, and arithmetic codes \cite{steinruecken_lossless_2014}. A simple example of a dictionary code for English is augmenting the Latin alphabet with a new symbol which means "the", reducing the number of symbols required from three symbols to one. Arithmetic coding, on the other hand, encodes a single codeword representing the entire message by explicitly representing the decisions required to construct it. It is typically much easier to change the modeling process when using arithmetic coding, as dictionary codes tend to be intertwined with the model. Therefore, we shall be using arithmetic coding. One approach to implementing an arithmetic coder, particularly attractive in the context of using Haskell, is given by \textcite{goos_arithmetic_2003}. \textcite{steinruecken_lossless_2014} also provides an interesting implementation.


\subsection{Directed information and network information theory}
Directed information theory (and, more generally, network information theory) is the study of information flow in stateful systems with feedback. Originating from work by \textcite{massey_causality_1990}, it is particularly useful for causal systems, where future symbols may depend on past symbols. It has been used to characterise protocol dialog \cite{weissman_directed_2011}, filter networks \cite{derpich_entropy_2015}, some aspects of compression and hypothesis testing \cite{permuter_interpretations_2009}, and even LGQ control \cite{tanaka_lqg_2015}. \textcite{quinn_directed_2012} shows a connection with probabilistic graphical models.





\chapter{Plan and progress}

\section{Plan}
Our high-level plan is the following. We are creating libraries and a test application in Haskell which simulates experimental data with a damped oscillator plus noise, and passing it on to physically-based regression applications. Then, using the resulting models, arithmetically code the data, and, if compression is poor, prompt for additional data. We measure our increasingly complex models in terms of coherence progress. 

The next step of the plan is determining the effects of our approach. We shall broaden the types of available simulated data (for example, a projectile) and attempt to train an autonomous agent via reinforcement learning with coherence progress as a reward metric. The methodology by which the agent performs experiments to input new data is the behaviour being trained, whether by studying new classes of data, or by changing experimental control variables.

Initially, the project was more concerned with the architecture of the agent used to analyse the benefits of our compression scheme. The focus has since shifted to the compressor itself, reducing scope and deferring decisions on the design of the agent until the second phase of implementation and testing.
\section{Initial design decisions}
\subsection{Implementation language}
The design is being implemented in Haskell. The two chief options were Haskell and Python. While Python has the advantage of being easy to start protoyping with, and native TensorFlow bindings, it is much more difficult to debug and refactor than Haskell. Haskell has the advantage of being a lazy functional programming language, meaning mathematically-oriented software is much easier to read and write. What cemented Haskell as the superior choice was our greater familarity with the language and it's various frameworks.

\subsection{Simulated data}
Time series data for several continuous models, such as damped oscillators and a thrown projectile, are easy to simulate. The existence of closed form solutions provide useful checks on the models created by our compressor.

\subsection{How to build models?}
The initial prototype is merely choosing between several hardcoded models. Once the overall framework is complete, this will be removed in favour of actually analysing the data. The two chief candidates were \autocite{duvenaud_automatic_2014} and \autocite{schmidt_distilling_2009}. \autocite{schmidt_distilling_2009} has the advantage of having already being implemented as a COTS software package (Eureqa), free for academic use, whereas \autocite{duvenaud_automatic_2014} may allow more flexibility and not having to spend time learning the API of the software package. Currently, only Eureqa is planned on being used.


\section{Outline of compressor}
The overall design is based on a computational effects framework \cite{kiselyov_freer_} in order to minimise the interaction between logically separate; for example, the random number generator doesn't need to worry about I/O. Computational effects are rather suited to a pipeline style of processing.
An overview of the data flow is shown in \fref{fig:CompressorOverview}.
\subsection{Components}
\subsubsection{
\subsection{Current problems}

\begin{figure}
\centering\includegraphics{CompressorV1Overview.png}
\caption{Overview of prototype compressor}
\label{fig:CompressorOverview}
\end{figure}

\section{Questions remaining about the test agent}
The following questions must be answered before implementation of the relevant modules.
\subsection{How should decisions be made regarding what action to take?}
Possible options include neural networks, or a topological approach such as outlined by \textcite{lakkaraju_identifying_2016}. 
\subsection{What reinforcement learning algorithm should be chosen?}
Episodic learning is a seemingly appropriate technique to employ, enabling an agent to "look back" at past episodes in order to "strengthen their reflexes". The specific choice depends on the technique used for deciding actions.
\subsection{How can the agent move through the action space?}
The primary action of the agent is to perform experiments. Thus, actions correspond to experiments with specific control variables. 
\subsection{How can the agent perform experiments?}
Control variables for continuous data corresponds to the base topological space of a fibre bundle. The time series is the fibre at that point.

\subsection{When should we build new models?}
Should building new models be part of the compressor that happens automatically, or should it be an action exposed to the reinforcement learning algorithm? 


\chapter{Experiments and future investigation}
The overall goal is to experiment with how curiosity can be exploited throughout all aspects of a robotic cosmonaut's design, not just its cognitive architecture. 
\section{Can data compressed during learning be decompressed losslessly, within reason?}
While the main goal of a robotic scientist is to understand the world, we would like to see the data supporting its beliefs; in addition to verifying model correctness, the data may be valuable in its own right. Thus, it is desirable for the data to be transmitted to human scientists such that they can actually retreive it.
\section{Can the compressed data be sent to human scientists without further processing?}
Is the data compressed by the agent in a format and size suitable for transmission and use by human scientists, or must it be further compressed? This serves as a sanity check on the capability of the agent to produce a meaningful model, as the better the model is at predicting the data, the better the compression.
\section{Can meaningful models be extracted?}
As the goal of science is to build models of the universe in order to deepen our understanding, we would like to be able to understand the learned models. Compared to non-trivial deep neural nets, which are notoriously opaque to human understanding, it is desirable for the agent to create sparse models in the form of some symbolic language.
\section{Background knowledge versus tabula rasa}
How much prior knowledge should be given? Generally, the more domain-specific compression models are, the more effective they are, but the more assumptions they build on. Clearly, this is a problem when trying to discover new phenomena. Providing an initial model may be more harmful than starting with a blank slate, as it may first have to unlearn the model. An example may be moving from a geocentric model of the solar system to a heliocentric model. Alternatively, it may make it hard to break past an abstraction, for example going from Newtonian gravity to general relativity. Of course, this depends on the capabilities of the chosen method of automatic model creation. As such, we shall investigate the performance of the agent when given basic knowledge of physics, such as Newton's laws and some constants, and compare its performance against a blank slate, a highly unphysical model, and only very fundamental physical laws such as Noether's theorem.

\section{How can a computer algebra system be integrated with model discovery?}
Computer algebra systems such as Sage (\cite{sagemath}) may be of considerable use when analysing data. How could it be integrated into our compression scheme? We leave this for future research.

\section{Can we describe our hypotheses, actions and observations in modal logic?}
Formalising our thought history in logic may help to find better models. For example, we may be able to pose our questions to an SMT solver such as Z3 \autocite{de_moura_z3_2008}. Finding a modality to capture the notions of "interesting" and "understanding" would presumably be required. Action modalities, such as observation and hypothesis testing, are likely to be straightforward to express in terms of dynamic logic. We leave this for future research.

\printbibliography 
\end{document}
