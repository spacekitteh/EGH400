
[2] Space exploration is hard. One of the reasons for this is the distance. Despite the speed of light being incredibly fast to us, the universe is very big. The radio propagation time between Earth and a rover on Mars, for example, can be anywhere between 4 minutes and 24 minutes, depending on the time of (each planet's) year. 

The distance also means enormous free space path loss - For example, an S band transmission experiences between 257 and 274 dB of loss. This greatly reduces the Shannon capacity of the communication channel, reducing the amount of science data which can be returned.

----

[3] Thus, autonomy is crucial in overcoming these barriers. We can create relatively high level, autonomous GNC controllers which we issue high-level instructions such as "Go to that interesting rock", and it can be expected to avoid getting stuck in a crater enroute without any human interaction. 

We can also preprocess the collected data, such as creating and transmitting only a histogram of the data, rather than the individual datapoints. Of course, we can also compress it.

----

[4] There are two main, complimentary approaches to the creation of autonomous systems: Control theory, which is suited to low-level systems when both the plant and desired behaviour are relatively easy to mathematically model, and so-called artificial intelligence, which is suitable when we find great difficulty in modeling the plant, but can still describe the desired behaviour.

It is desirable to make our robotic cosmonauts as autonomous as possible, so ideally we could just instruct them to "go do some science and report any interesting discoveries". But how can we mathematically model "Go do some science", or, for that matter, "interesting discovery"?

----

[5] Let's describe science as "systematic curiosity". But what is curiosity? We may describe it as acting in order to maximise the interestingness of our observations.

We aren't interested in things which are completely predictable, nor about things which appear completely random.

----

[6] When we see probability turn up in abstract concepts, such as "interaction", "understanding" or "interesting", it is helpful to turn to information theory.

Thus, interestingness is a nonmonotonic measure of similarity to previously understood, or modeled, information.

Compression, the study of which is field of information theory, works by finding and exploiting similarity in the given data by creating probabilistic models. This is exactly why we want the data - to study it to discover and model the physical processes which create it.

----

[7] This means we can use compression to formalise curiosity, in the form of, for example, coherence progress. Roughly speaking, this is the derivative of how effective our models are as we add new data: If two consecutive results are very efficiently compressed, then the difference in effectiveness is low, thus coherence progress is low; if two consecutive results are poorly understood and modeled, then they will not compress well, and the change in effectiveness is low as well. But if the adding of a new result allows the discovery of a new, more effective model, then the difference is large. Thus, we can see coherence progress captures the nonmonotonic nature of interestingness.

{{{{{TIME: 3:25}}}}}}
----

[8] Most extant compression schemes don't actively ask for additional data (meaning, in our case, perform more experiments) to improve its models. We have thus chosen to investigate pro-active, physically-based compression schemes for use in autonomous agents.


----

[9] Our high-level plan is the following. We shall create a test application in Haskell which simulates experimental data with a damped oscillator plus noise, and passes it on to physically-based regression applications. Then, we'll take the resulting models, arithmetically code the data, and, if compression is poor, prompt for additional data. We measure our increasingly complex models in terms of coherence progress. 

Then, we shall broaden the types of available simulated data (for example, a projectile) and attempt to train an autonomous agent via reinforcement learning with coherence progress as a reward metric. The methodology by which the agent performs experiments to input new data is the behaviour being trained, whether by studying new classes of data, or by changing experimental control variables.

----

[10] The simulated data is easy to produce; just a couple of lines of code. Much study and consideration of approaches to the design of the compressor has been undertaken, and implementation has just begun. It is believed that a computational-effects-based approach will be the most fruitful. This allows, among other things, the creation of models to be very modular. The very first prototype will just decide between two hard-coded models; much of work will be on interfacing to the model discovery program, Eureqa, which is based on a genetic algorithm.


----

[11] Generally, the more domain-specific compression models are, the more effective they are, but the more assumptions they build on. Clearly, this is a problem when trying to discover new phenomena. Thus, only very general physical principles, such as Noether's theorem or the principle of least action, should be provided. But this may prove too much of a leap for model discovery to be effective; thus, we shall also provide additional, derived, results, such as Newtonian and Hamiltonian mechanics. The key is to *allow* their use, but not *require* it. The ability to perform various mathematical techniques, such as integral transforms is left for future research. 

----

[12] If one allows a multi-pass approach to the compression, then it is straightforward to ask for, and incorporate, additional data and any new insights. However, this presents a memory problem: We don't want to keep many copies of the data around, due to the limited memory and susceptibility to corruption inherent to space systems. How to deal with this is left for future research.

----